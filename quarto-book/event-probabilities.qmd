# Event probabilities

```{python}
#| output: false
#| echo: false
from cmdstanpy import CmdStanModel
import numpy as np
import pandas as pd
import logging
cmdstanpy_logger = logging.getLogger("cmdstanpy")
cmdstanpy_logger.disabled = True
```

Events are the kinds of things that either happen or don't happen with
some probability.  Informally, we can think of events like it raining
today, a fair coin landing heads, or the Rays winning American
baseball's eastern division this year.  In this chapter, we formalize
this general idea of events.  

## Events

A general condition on a random variable is called an *event*.
Suppose $Z$ is a random variable representing the outcome of a fair
roll of two six-sided dice.  Examples of events include $Z$ being
equal to 7, $Z$ being less than or equal to 5, or $Z$ being odd.
Informally, an event $A$ is just a subset of possible outcomes for a
random variable.^[Defining the collection of events more formally
requires the notion of a $\sigma$-algebra from measure theory, which
is beyond the scope of this book.]  For example, the conditions on
random variables we considered above correspond to the following events.  

* $Z = 7$:  $\quad A = \{ 7 \}$
* $Z \leq 5 \ $: $\quad B = \{ 2, 3, 4, 5 \}$
* $Z \textrm{ odd}$: $\quad C = \{ 3, 5, 7, 9, 11 \}$

## Indicator functions

When dealing with events, we will be making heavy use of the so-called
*indicator function*, which converts boolean values (true or false) to
integers (1 and 0).  That is, we define the indicator function
$\textrm{I}$ for a boolean-valued condition $\phi \in \{
\textrm{true}, \textrm{false}\}$ by
$$
\textrm{I}(\phi)
= \begin{cases}
    1 & \textrm{ if } \phi \textrm{ is true, and}
    \\[2pt]
    0 & \textrm{ otherwise.}
\end{cases}    
$$

### Random indicators

Events are boolean conditions on random variables.  For example, when
we write the event $Y \leq 5$, it produces a random variable that
returns truth values, so that $\textrm{I}[Y]$ is the random variable
that returns 1 if $Y \leq 5$ and 0 otherwise.^[We again use square
brackets for functions applied to random variables.]

## Event Probability

The probability of an event defined as a condition on random
variables is defined as the expectation of the indicator function
applied to the condition.  In symbols, we write $\textrm{Pr}[A]$ for
the probability of an event $A$.  In general, when $\phi(Z)$ is a
condition on a random variable $Z$ defining an event, the *probability*
of the event is defined by 
$$
\textrm{Pr}[\phi(Z)] = \mathbb{E}[\textrm{I}[\phi(Z)]].
$$
Because the indicator function only takes on values 0 or 1, and
expectations are averages, it follows that probabilities fall between
0 and 1 (inclusive), i.e., for a random event $A$,
$$
\textrm{Pr}[A] \in [0, 1].
$$

Although we could calculate the probabilities of each outcome of
tossing two fair dice analytically, we first tackle the problem
through simulation.  To calculate expectations, we just calculate a
sample average, so if we have a condition $\phi(Z)$ and simulated
values $z^{(1)}, \ldots, z^{(M)}$ for $Z$, we cam estimate the event
probability for $\phi(Z)$ as
\begin{eqnarray*}
\textrm{Pr}[\phi(Z)]
& = & \mathbb{E}[\, \textrm{I}[\phi(Z)] \,]
\\[4pt]
& \approx & \frac{1}{M} \sum_{m=1}^M \textrm{I}(\phi(z^{(m)})).
\end{eqnarray*}
In words, the estimated probability is proportion of draws $z^{(m)}$
where the condition holds.

Let's return to our example of a random variable $Z$ corresponding to
the total on the roll of two fair dice and evaluate the probabilities
of the examples we provided.  We simulate the data as before and
extract the draws into a sequence `zs`.

```{python}
#| code-fold: true
model_dice = CmdStanModel(stan_file = "dice.stan")
fit_dice = model_dice.sample(seed=1234, show_progress=False, show_console=False)
z = fit_dice.draws_pd()["z"]
```

Then we can estimate the probabilities of our events as follows.  We
will first do it by the explicit loop that matches our mathematical
definition.  We start with the event $Z \leq 5$.

```{python}
occur = 0
for z_m in z:
  if z_m <= 5: occur += 1
p = occur / len(z)
print(f"Pr[Z <= 5] = {p:4.2f}")
```

When we have NumPy arrays, we can do this counting as a one-linear
using vectorized comparisons and summation.  For example, if we have a
NumPy array and apply a condition to it, we get a NumPy array of
boolean values.

```{python}
a = np.array(range(2, 13))
print(f"{a <= 5 = }")
```

We can then sum those values to get a count of values that are less
than or equal to five.

```{python}
print(f"{sum(a <= 5) = }")
```

The result is the number of `True` values.  This works by casting the
boolean values to integer values (`True` to 1 and `False` to 0) so
that the sum counts the number of `True` values.  We can then divide
these counts by lengths to estimate the probabilities of our example
events.  

```{python}
p1 = sum(z == 7) / len(z)
p2 = sum(z <= 5) / len(z) 
p3 = sum(z % 2 == 1) / len(z)
print(f"Pr[Z = 7] = {p1:4.2f}")
print(f"Pr[Z <= 5] = {p2:4.2f}")
print(f"Pr[Z is odd] = {p3:4.2f}")
```

## Event probabilities with Stan

Rather than filtering and counting a sequence of draws, we can
calculate event probabilities using Stan.  Because Stan's default
summary gives us the sample mean of all of the variables in the
program, all we need to do is define indicators for events of
interest.  Here's a Stan program to compute our example events.

`dice-events.stan`
```stan
generated quantities {
  int<lower=1, upper=6> x = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=2, upper=12> z = x + y;
  int<lower=0, upper=1> z_eq_7 = (z == 7);
  int<lower=0, upper=1> z_lte_5 = (z <= 5);
  int<lower=0, upper=1> z_odd = (z % 2 == 1);
}
```


```{python}
#| code-fold: true
model_dice_events = CmdStanModel(stan_file = "dice-events.stan")
fit_dice_events = model_dice_events.sample(seed=1234, show_progress=False, show_console=False)
fit_dice_events.summary(sig_figs=3, percentiles=(50,))
```

The event probability estimates are just the means for the relevant
indicator variables.  We see that they match the values we computed by
hand for our example events.  We can also see that the expected value
of $Z = X + Y$ is the expected value of $X$ plus the expected value of
$Y$.

