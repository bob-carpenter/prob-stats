# Laws of Probability

Now that we have events, which are the bearers of probability in
probability theory, we are ready to introduce the basic algebraic laws
of probability.  We have presented probability theory as a *fait
accompli* up to this point, but we will now pull the curtain back and
introduce the three basic axioms of probability theory.

An event $A$ is just a set of possible outcomes, with $\textrm{Pr}[A]
\in [0, 1]$ being the probability of event $A$. The set of all
possible outcomes is called the *sample space*.  For example, if we
are considering a single finite random variable taking on values in
2--12, then the sample space is $S = \{ 2, 3, \ldots, 12 \}$.  If $Y$
is a random variable taking on values in $S$, such as the sum of the
roll of two dice, then an event such as $Y \leq 3$ picks out the event
$A = \{ 2, 3 \}$.  Every event is a subset of the sample space, $A
\subseteq S$.  The entire sample space $S$, for example, is an event.

## Axioms of Probability

@kolmogorov1950foundations introduced
measure-theoretic models of probability that
demonstrated the consistency of his three axiom sof probability.


### Axiom 1: Event probabilities are non-negative

The first axiom requires probabilities to be non-negative. 
$$
\textrm{Pr}[A] \geq 0 \textrm{ for all events } A.
$$

The first axiom is consistent with our informal definition of an event
probability as the expectation of an indicator over events.  Because
indicators take on value 0 or 1, their expectation, or average, must
also take on a value between 0 and 1.


### Axiom 2: The sample space has probability unity

The second axiom requires the event consisting of the entire sample
space $S$ to have unit probability.
$$
\textrm{Pr}[S] = 1.
$$

The second axiom is also consistent with events defined through
indicator functions on random variables.  For example, if $Y$ is a
discrete random variable, then the event $Y \in \mathbb{N}$ is such
that $y^{(m)} \in \mathbb{N}$ is true for any simulated value
$y^{(m)}$ of $Y$.  Therefore, $\textrm{I}(y^{(m)} \in \mathbb{N}) =
1$, so that for any sample $y^{(1)}, \ldots, y^{(M)}$,

$$
\frac{1}{M} \sum_{m=1}^M \textrm{I}(y^{(m)} \in \mathbb{N})
\ = \
\frac{1}{M} \sum_{m = 1}^M 1
\ = \
\frac{1}{M} M
\ = \ 1.
$$


### Axiom 3: Disjoint events have additive probability

The third axiom of probabilities requires 
$$
\textrm{Pr}\left[ \bigcup_{n=1}^{\infty} A_n \right]
    = \sum_{n=1}^{\infty} \textrm{Pr}[A_n]
$$
whenever $A_1, A_2, \ldots$ is a pairwise disjoint sequence of events.
A pair of sets $A$ and $B$ is *disjoint* if their intersection is
empty, i.e., $A \cap B = \emptyset$.  A sequence of sets $A_1, A_2,
\ldots$ is *pairwise disjoint* if $A_i$ and $A_j$ are disjoint for all
$i \neq j$.

The third axiom enforces *countable additivity* in the sense that the
sequence $A_1, A_2, \ldots$ is countable.  Countability additivity
implies *finite additivity*, which says that if events
$A$ and $B$ are disjoint, then
$$
\textrm{Pr}[A \cup B] = \textrm{Pr}[A] + \textrm{Pr}[B].
$$

## Basic laws of probability

From the three simple axioms of probability, we can prove a range of
additional results.

### Complement law

If $A$ is an event, then
$$
\textrm{Pr}[A^\complement] = 1 - \textrm{Pr}[A].
$$
This law follows from the first and last line of
\begin{eqnarray*}
\textrm{Pr}[A] + \textrm{Pr}[A^\complement]
& = & \textrm{Pr}[A \cup A^\complement]
\\[4pt]
& = & \textrm{Pr}[S]
\\[2pt]
& = & 1.
\end{eqnarray*}

Given that the empty set $\emptyset = S^\complement$ is the complement of
the universal set $S$, and $\textrm{Pr}[S] = 1$, it follows
from the complement law that
$$
\textrm{Pr}[\emptyset] = 0.
$$

If $A$ is an event, then $A$ and $A^\complement$ are disjoint, and $A \cup
A^\complement = S$.  It then follows because
$\textrm{Pr}[A^\complement] \geq 0$ and $\textrm{Pr}[A] = 1 -
\textrm{Pr}[A^\complement]$, that 
$$
\textrm{Pr}[A] \leq 1.  
$$

### Set theoretic laws

If $A$ and $B$ are events, then 
$$
\textrm{Pr}[A \cup B]
= \textrm{Pr}[A] + \textrm{Pr}[B] - \textrm{Pr}[A \cap B].
$$

If $A$ and $B$ are events, then
$$
\textrm{Pr}[B \cap A^\complement] = \textrm{Pr}[B] - \textrm{Pr}[A \cap B].
$$

If $A$ and $B$ are events, then
$$
A \subseteq B \ \textrm{implies} \ \textrm{Pr}[A] \leq \textrm{Pr}[B].
$$

### Partition law

If $C_1, C_2, \ldots$ is a partition of the sample space $S$ (i.e.,
the $C_i$ are pairwise disjoint and $S = \bigcup_{n < \infty} C_n$), and $A$
is an event, then
$$
\textrm{Pr}[A]
= \sum_{n = 0}^\infty \textrm{Pr}[A \cap C_n].
$$

