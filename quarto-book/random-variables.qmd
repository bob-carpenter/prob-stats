# Random Variables

Stan is a probabilistic programming language in the sense that its
variables can be random as well as deterministic.  Each time Stan is
run, these random variables can take on different values.

## Simulating a fair coin flip

We will start with the canonical "Hello, world!" example for
statistics, a coin flip.

### Stan program for a coin flip

Here's the Stan program.  When we list Stan
programs, we title them with the file name in which they can be found
in the source distribution.

##### `flip.stan`
```stan
generated quantities {
  int<lower=0, upper=1> y = bernoulli_rng(0.5);
}
```

This trivial Stan program assigns the variable `y` randomly, with a
50% chance of a 1 value and 50% chance of a 0 value.  The program
begins with a *block declaration*.  The *generated quantities block*
here is used to generate random quantities based on the state of other
program variables.  The variable `y` is declared to be an integer with
the type `int` and constrained to fall between 0 and 1 (inclusive)
with `lower=0` and `upper=1`.  The variable's value is the result of
executing the function `bernoulli_rng` applied to the argument
expression `0.5`.  The value of `bernoulli_rng` is random, returning 1
with the specified probability of 0.5 (a 50% chance) and returning 0
with probability 0.5.


### Executing Stan through Python

We use Python to run the code and show the result of the simulated
coin flip.  First, we are going to import cmdstanpy and then turn off
its intrinsic logger to avoid cluttering our output with feedback messages intended for the console.


```{python}
#| output: false
from cmdstanpy import CmdStanModel

import logging
cmdstanpy_logger = logging.getLogger("cmdstanpy")
cmdstanpy_logger.disabled = True
```

Next, we compile the Stan model from its source `flip.stan` using the
constructor for the class `CmdStanModel`.

```{python}
model = CmdStanModel(stan_file = "flip.stan")
```

Next, we use the `sample()` method on the model class to sample values
from the program.

```{python}
fit = model.sample(seed=1234,
                   show_progress=False,
                   show_console = False)
```

The `seed` argument determines a random seed.  If we rerun a Stan
program with the same random seed, we get the same sequence of
pseudo-random simulation values.


### Extracting fitted results

After fitting a model, we can extract the draws for `y` from the `fit`
object.

```{python}
df = fit.draws_pd()
y = df['y']
```

For example, here's the result of the first draw.

```{python}
print("first flip = {0:.0f}".format(y[0]))
```

Continuing past the first simulation, here are the first dozen
simulated values.


```{python}
for i in range(12):
    print("y({0:d}) = {1:.0f}".format(i, y[i]), end=", ")
    if i == 5: print("")
print("...")
```

We see that some of the simulated values are 1 and some are 0.  These
do not correspond to flipping a dozen different coins or the same coin
a dozen times.  Instead, the simulated values represent different
possible values for a single flip of a single coin.


## Simulating a fair die

The same way we could generate two variables uniformly, we can
define a random variable whose value falls between 1 and 6 randomly.  Here is the Stan code.

`die.stan`
```stan
generated quantities {
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
}
```

Here, we have a lower bound of 1 and upper bound of 6 (inclusive), and
we use a categorical random number generator that assigns a
probability of `1 / 6` to each outcome (we write the numbers using
decimal places to avoid reduction to integer arithmetic, which
rounds toward zero).

```{python}
#| code-fold: true
model_die = CmdStanModel(stan_file = "die.stan")
fit_die = model_die.sample(seed=1234, show_progress=False, show_console=False)
z = fit_die.draws_pd()["z"]
for i in range(10):
  print("z({0:d}) = {1:.0f}".format(i, z[i]), end=", ")
  if i == 4: print("")
print("...")
```


## Functions on random variables

If $X$ is a random variable and $Y$ is a random variable, then so is
$Z = X + Y$.  We can simulate values for $Z$ by simulating a value for
$X$ and simulating a value for $Y$ and then adding them.  For
instance, if we want the result of rolling two dice, we can do that
with the following Stan program.

`dice.stan`
```stan
generated quantities {
  int<lower=1, upper=6> x = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=2, upper=12> z = x + y;
}
```

The Stan program defines two variables, `x` and `y`, modeling them
both as a fair die, and defines the variable `z` to be their sum.
Here are 20 simulations of `z`.

```{python}
#| code-fold: true
model_dice = CmdStanModel(stan_file = "dice.stan")
fit_dice = model_dice.sample(seed=1234, show_progress=False, show_console=False)
z = fit_dice.draws_pd()["z"]
for i in range(20):
  print("z({0:2d}) = {1:2.0f}".format(i, z[i]), end=", ")
  if (i + 1) % 5 == 0: print("")
print("...")
```

While every outcome of throwing a single fair die is equally likely,
this is no longer ture when throwing two dice and taking their sum.
As you can see from the example, the outcomes 5, 6, 7, and 8 occur
more than values like 2 and 12.

### Estimating probabilities

Although we could calculate the probabilities of each outcome of
tossing two fair dice analytically, we first tackle the problem
through simulation.  With simulation, all we need to do is run
multiple simulations and look at simulated frequencies.  We assume we
start with draws `z` from the previous simulation.

```{python}
fit_dice = model_dice.sample(seed=1234, show_progress=False, show_console=False)
z = fit_dice.draws_pd()["z"]

from collections import Counter
counts = Counter(z)
for n in range(2, 13):
  print("Pr[Z = {0:2d}] = {1:4.2f}".format(n, counts[n] / len(z)), end="   ")
  if ((n - 1) % 3 == 0): print("")
```

In our output, we have implicitly introduced *event probability*
notation.  For example, we write $\textrm{Pr}[Z = 3]$ for the
probability that the random variable $Z$ takes on the value 3.

We can plot this output using a bar chart.  

```{python}
#| label: two_d_six_probs
#| fig-cap: "Probabilities of outcomes for rolling two six-sided dice."
import numpy as np
import pandas as pd
import plotnine as pn
probs = []
for n in range(2, 13):
  probs.append(counts[n] / len(z))

roll_probs = { "roll": range(2, 13),
               "probability": probs }

df = pd.DataFrame(roll_probs)

plot = pn.ggplot(df, pn.aes(x = 'roll', y = 'probability')) \
  + pn.geom_bar(stat="identity") \
  + pn.scale_x_continuous(breaks=range(2, 13))
plot.draw();  # semicolon prevents unwanted text echo
```

## Simulating stick breaking

Imagine we have a stick that's twenty units long and we break it in a
random place.  That corresponds to generating a continuous random
number in the range $[0, 20]$.  We can simulate this random process with Stan as follows.

`stick.stan`
```stan
generated quantities {
  real<lower=0, upper=20> alpha = uniform_rng(0, 20);
}
```

In contrast to our previosu example, we declare our variable to be real-valued with the keyword `real`.  We also use a uniform random number generator, which generates a real number in the specified range at random.

We will compile the model and sample in a single code block this time.

```{python}
model_stick = CmdStanModel(stan_file = "stick.stan")
fit_stick = model_stick.sample(seed=1234, show_progress=False, show_console=False)
alpha = fit_stick.draws_pd()["alpha"]
for i in range(5):
  print("alpha({0:d}) = {1:5.2f}".format(i, alpha[i]))
```

We have only printed the first two decimal places of each number, but we can see that we are generating real values between 0 and 20.




## Random variables

In probability theory notation, random variables are conventionally
written as capital letters.  For example, we might let $Y$ be the
random variable corresponding to the result of a specific fair coin
flip.  Values for random variables are conventionally written using
lower case.  For example, $y = 1$ is a particular value of the
random variable $Y$.

The variable `y` in our sample program is written in lower case
because Stan computes with concrete values.  When we run a Stan
program, it produces a sequence of simulated values for each random
variable in the program.  We index sequences of values with
parenthesized superscripts.  For example, we might write $M$ simulated
values of the random variable $Y$ as $y^{(0)}, \ldots, y^{(M-1)}$.

### Random seeds

In our sampling code, we fixed a random seed, `1234`, which determines
the sequence of random numbers generated In practical sampling code,
this means its value is dependent on the state of the underlying
random number generator.  This mirrors the way random variables are
defined in measure theory as functions from sample spaces to values.  



## Expectation, variance, and standard deviation

The *expectation* of a random variable is defined as its average
value. For example, if $Y$ is a random variable, we write its
expectation as $\mathbb{E}[y]$.  We can calculate approximate
expectation values by averaging samples.  If $y^{(0)}, \ldots,
y^{(M-1)}$ is a sequence of $M$ simulated values for $Y$, then the *expectation* of 
$$
\mathbb{E}[Y] \approx \frac{1}{M} \sum_{m < M} y^{(m)}.
$$


### Variance and standard deviation

The *variance* of a random variable is defined as its average squared
difference from its expected value.  This is also an expectation.
$$
\textrm{var}[Y] = \mathbb{E}[(Y - \mathbb{E}[Y])^2] \approx \frac{1}{M} \sum_{m < M} (y^{(m)} - \mathbb{E}[Y])^2.
$$
The units of variance are the units of $Y$ squared, which makes it inconvenient for informal reasoning.

The *standard deviation* of a random variable is defined to be the
square root of its variance.
$$
\textrm{sd}[Y] = \sqrt{\textrm{var}[Y]}.
$$


### Quantiles


If $\alpha \in [0, 1]$, we say that the $\alpha$-*quantile* of a the
random variable $Y$ is the value $y$ such that the probability that
$Y$ is less than or equal to $y$ is $\alpha$.  That is, $y$ is the
$\alpha$-quantile of $Y$ if $\textrm{Pr}[Y < y] = \alpha$.  We can estimate the $\alpha$-quantile through sampling as
$$
\textrm{quantile}(Y, \alpha) \approx y^{(\lfloor \alpha \cdot M \rfloor)}.
$$

