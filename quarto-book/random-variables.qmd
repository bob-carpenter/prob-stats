# Discrete Random Variables

## Discrete variables and natural numbers

A *random variable* is one that could take on different values. For
example, we can represent the result of a coin flip as a random
variable. We can also represent the location of a dart throw relative
to the bullseye as a random variable.

In this chapter, we explore random variables that are discrete.
A *discrete variable* is one that takes on values that can be
represented as a subset of the *natural numbers*,
$$
\mathbb{N} = \{ 0, 1, 2, \ldots \}.
$$
Rather than saying the outcome of a coin flip is "heads" or "tails",
we will *code* heads as 1 and tails as 0.

A *finite variable* is a discrete variable whose values can be encoded
using the first $N$ natural numbers, $\{ 0, 1, \ldots, N - 1 \}$, for
some $N \in \mathbb{N}$. A variable that is not finite is said to
be *infinite*.

## Simulating a fair coin flip

We will start with the canonical "Hello, world!" example for
statistics, a coin flip.


### Stan program for a coin flip

Here's the Stan program for a coin flip.  We title Stan programs in
the book with the file in which they can be found in the source distribution.

##### `flip.stan`
```stan
generated quantities {
  int<lower=0, upper=1> y = bernoulli_rng(0.5);
}
```

Stan is a probabilistic programming language in the sense that its
variables can be random as well as deterministic.  The variable `y`
here is assigned the value 1 with probabilty 0.5 and the value 0 with
probability 0.5.

Breaking the Stan program down, it starts with a `generated
quantities` block declaration. The *generated quantities* block is
used to generate random quantities based on other values. A
Stan *program* is made up of a sequence of blocks, each of which is
made up of a sequence of statements.

The block here contains a single *statement*, which *declares* the
*type* of the *variable* `y` and also *defines* its value. Every
variable in Stan has a single type that is determined *statically*
(i.e., before the program is run). In this case, the type of `y` is
declared to be an integer with the keyword `int`. The value of `y`
is *constrained* to fall between the specified lower and upper bounds.
The lower and upper bounds are given by the numeric *literals*, `0`
and `1`. The equal sign (`=`) is used to denote assignment, with the
variable `y` being assigned the value of executing the *expression*
`bernoulli_rng(0.5)`. The expression consists of a *function*
`bernoulli_rng` and an argument, the numeric literal `0.5`. The value
of `bernoulli_rng` is random, returning $1$ with the specified
probability of $0.5$ (a $50\%$ chance) and returning $0$ with probability
$1 - 0.5$ (also a $50\%$ chance).


### Executing Stan through Python

We use Python to run the code and show the result of the simulated
coin flip.  First, we are going to import cmdstanpy and then turn off
its intrinsic logger to avoid cluttering our output with feedback messages intended for the console.

```{python}
#| output: false
from cmdstanpy import CmdStanModel
import logging
cmdstanpy_logger = logging.getLogger("cmdstanpy")
cmdstanpy_logger.disabled = True
```

Next, we compile the Stan model from its source `flip.stan` using the
constructor for the class `CmdStanModel`.

```{python}
model = CmdStanModel(stan_file = "flip.stan")
```

Next, we use the `sample()` method on the model class to sample values
from the program.

```{python}
fit = model.sample(seed=1234,
                   show_progress=False,
                   show_console = False)
```

With default settings, Stan's sampler simulates a sequence of values
for all of the random variables in the program. The `seed` argument
determines a random seed. If we rerun a Stan program with the same
random seed, we get the same sequence of pseudo-random simulation
values.


### Extracting fitted results

After fitting a model, we can extract the draws for `y` from the `fit`
object.

```{python}
df = fit.draws_pd()
y = df['y']
```

For example, here's the result of the first draw.

```{python}
print(f"first flip = {y[0]:.0f}")
```

Continuing past the first simulation, here are the first dozen
simulated values.


```{python}
for i in range(12):
    print(f"y({i:d}) = {y[i]:.0f}", end=", ")
    if i == 5: print("")
print("...")
```

We see that some of the simulated values are 1 and some are 0.  These
do not correspond to flipping a dozen different coins or the same coin
a dozen times.  Instead, the simulated values represent different
possible values for a single flip of a single coin.


## Simulating a fair die

The same way we could generate two variables uniformly, we can
define a random variable whose value falls between 1 and 6 randomly.  Here is the Stan code.

`die.stan`
```stan
generated quantities {
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
}
```

Here, we have a lower bound of 1 and upper bound of 6 (inclusive), and
we use a categorical random number generator that assigns a
probability of `1 / 6` to each outcome (we write the numbers using
decimal places to avoid reduction to integer arithmetic, which
rounds toward zero).

```{python}
#| code-fold: true
model_die = CmdStanModel(stan_file = "die.stan")
fit_die = model_die.sample(seed=1234, show_progress=False, show_console=False)
z = fit_die.draws_pd()["z"]
for i in range(10):
  print(f"z({i:d}) = {z[i]:2.0f}", end=", ")
  if i == 4: print("")
print("...")
```


## Functions on random variables

If $X$ is a random variable and $Y$ is a random variable, then so is
$Z = X + Y$.  We can simulate values for $Z$ by simulating a value for
$X$ and simulating a value for $Y$ and then adding them.  For
instance, if we want the result of rolling two dice, we can do that
with the following Stan program.

`dice.stan`
```stan
generated quantities {
  int<lower=1, upper=6> x = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=2, upper=12> z = x + y;
}
```

The Stan program defines two variables, `x` and `y`, modeling each as
a fair die, and defines the variable `z` to be their sum.  Here are 20
simulations of `z`.

```{python}
#| code-fold: true
model_dice = CmdStanModel(stan_file = "dice.stan")
fit_dice = model_dice.sample(seed=1234, show_progress=False, show_console=False)
z = fit_dice.draws_pd()["z"]
for i in range(20):
  print(f"z({i:2d}) = {z[i]:2.0f}", end=", ")
  if (i + 1) % 5 == 0: print("")
print("...")
```

While every outcome of throwing a single fair die is equally likely,
this is no longer true when throwing two dice and taking their sum.
As you can see from the example, intermediate outcomes like 7 and 8 occur
more frequently than extreme values like 11 and 12.


## Event probabilities

A general condition on a random variable is called an *event*.
Suppose $Z$ is a random variable representing the outcome of a fair
roll of two six-sided dice.  Events include examples such as $Z$ being
equal to 7, or being less than or equal to 5, or being odd.
Informally, an event $A$ is just a subset of possible outcomes for a
random variable.^[Defining the collection of events more formally
requires the notion of a $\sigma$-algebra from measure theory, which
is beyond the scope of this book.]  For example, the conditions on
random variables we considered above correspond to events $A$,
$B$, and $C$, where follows. 

* $Z = 7$:  $\quad A = \{ 7 \}$,
* $Z \leq 5 \ $: $\quad B = \{ 2, 3, 4, 5 \}$, and 
* $Z \textrm{ odd}$: $\quad C = \{ 3, 5, 7, 9, 11 \}$.

If $E$ is an event, we will write $\textrm{Pr}[E]$ for the
*probability* of that $E$ occurs.^[We use the higher-order function
notation of square brackets because events are sets of outcomes.]

In the case of dice, probabilities are calculated as the sum of the
probabilities of the individidual outcomes.  For example,
$$
\textrm{Pr}[Z \leq 5]
= \textrm{Pr}[Z = 2] + \textrm{Pr}[Z = 3] + \textrm{Pr}[Z = 4] + \textrm{Pr}[Z = 5].
$$
Probabilities are scaled so that they so that they fall between 0 and
1 inclusive, i.e., $\textrm{Pr}[A] \in [0, 1]$.

## Estimating probabilities with simulation

Although we could calculate the probabilities of each outcome of
tossing two fair dice analytically, we first tackle the problem
through simulation.  With simulation, all we need to do is run
multiple simulations and look at simulated frequencies.

We begin with draws $z^{(1)}$, \ldots, $z^{(M)}$ of $Z$ corresponding
to a fair roll of two six-sided dice. To estimate the probabilty of an
outcome, for example $\textrm{Pr}[Z = 7]$, we can use the proportion
of draws in which $z^{(m)} = 7$.  More formally, we will write this as
$$
\textrm{Pr}[Z = 7] = \frac{1}{M} \sum_{m=1}^M \, \textrm{I}(z^{(m)} = 7),
$$
where the *indicator function* $\textrm{I}$ is defined by
$$
\textrm{I}(\phi)
= \begin{cases}
    1 & \textrm{ if } \phi \textrm{ is true, and}
    \\[2pt]
    0 & \textrm{ otherwise.}
\end{cases}    
$$    

We can calculate these estimates for all values of $Z$ by looping over
the draws in the sample and incrementing the relevant count.

```{python}
import numpy as np
counts = np.zeros(13)         # initialize to 0
for z_m in z:                 # loop over draws
  counts[int(z_m)] += 1       # increment count for draw
pr_z_eq = counts / len(z)     # normalize by dividing by number of draws
```

Printing the results, we see that not every outcome is equally probable.

```{python}
#| code-fold: true
for n in range(2, 13):
  print(f"Pr[Z = {n:2d}] = {pr_z_eq[n]:5.3f}", end="   ")
  if ((n - 1) % 3 == 0): print("")
```

A more efficient approach is to use a `Counter` followed by a list
comprehension to carry out the arithmetic.

```{python}
#| code-fold: true
from collections import Counter
counts = Counter(z)
M = len(z)
pr_z_eq = [counts[n] / M for n in range(2, 13)]
```

In @fig-two-d-six We can plot our output using a bar chart, where the heights of the
bars are proportional to the probability of the outcome listed on the
axis.  

```{python}
#| label: fig-two-d-six
#| fig-cap: "Probabilities of outcomes for rolling two six-sided dice"
#| code-fold: true
import numpy as np
import pandas as pd
import plotnine as pn
probs = []
for n in range(2, 13):
  probs.append(counts[n] / len(z))

roll_probs = { "roll": range(2, 13),
               "estimated probability": probs }

df = pd.DataFrame(roll_probs)

plot = pn.ggplot(df, pn.aes(x = 'roll', y = 'estimated probability')) \
  + pn.geom_bar(stat="identity", color="black", fill="gray") \
  + pn.scale_x_continuous(breaks=range(2, 13))
plot.draw();  # semicolon prevents unwanted text echo
```

We write "estimated probability" on the vertical axis of this plot to
emphasize that we are using computational methods to approximate the
true value rather than analytically computing it with higher
precision.

### Calculating dice probabilities analytically

In the case of two dice, it's straightforward to calculate the
probability of each outcome analytically.  Because we have two
six-sided dice, there are 36 possible combinations $(d_1, d_2)$, where
$d_1$ is the result of the first die and $d_2$ the second. Each
outcome $(d_1, d_2)$ is equally probable, so the total probability of
an outcome is the number of ways it can be produced divided by the
total number of outcomes.  Table @tbl-2d6 shows the probabilities for
results 2--7.

| result | combinations  | probability |
|:-:|:-:|:-:|
| 2 | (1, 1) | 1/36 |
| 3 | (1, 2) &nbsp;  (2, 1) | 2/36 |
| 4 | (1, 3) &nbsp;  (2, 2) &nbsp;  (3, 1) | 3/36 |
| 5 | (1, 4) &nbsp;  (2, 3) &nbsp;  (3, 2) &nbsp;  (4, 1) | 4/36 |
| 6 | (1, 5) &nbsp;  (2, 4) &nbsp;  (3, 3) &nbsp;  (4, 2) &nbsp;  (5 1) | 5/36 |
| 7 | (1, 6) &nbsp;  (2, 5) &nbsp;  (3, 4) &nbsp;  (4, 3) &nbsp;  (5, 2) &nbsp;  (6, 1) | 6/36 |
: Combinations of two six-sided dice leading to result (their sum), and the associated probability. Results greater than 7 are symmetric, with 8 matching 6, 9 matching 5, $\ldots$, and 12 matching 2. {#tbl-2d6}

These results are very close to our estimated results, though it is
evident the simulation provides only about two significant digits of
accuracy compared to the analytical result.




## Simulating stick breaking

Imagine we have a stick that's twenty units long and we break it in a
random place.  That corresponds to generating a continuous random
number in the range $[0, 20]$.  We can simulate this random process with Stan as follows.

`stick.stan`
```stan
generated quantities {
  real<lower=0, upper=20> alpha = uniform_rng(0, 20);
}
```

In contrast to our previosu example, we declare our variable to be real-valued with the keyword `real`.  We also use a uniform random number generator, which generates a real number in the specified range at random.

We will compile the model and sample in a single code block this time.

```{python}
model_stick = CmdStanModel(stan_file = "stick.stan")
fit_stick = model_stick.sample(seed=1234, show_progress=False, show_console=False)
alpha = fit_stick.draws_pd()["alpha"]
for i in range(5):
  print(f"alpha({i:d}) = {alpha[i]:5.2f}")
```

We have only printed the first two decimal places of each number, but we can see that we are generating real values between 0 and 20.




## Random variables

In probability theory notation, random variables are conventionally
written as capital letters.  For example, we might let $Y$ be the
random variable corresponding to the result of a specific fair coin
flip.  Values for random variables are conventionally written using
lower case.  For example, $y = 1$ is a particular value of the
random variable $Y$.

The variable `y` in our sample program is written in lower case
because Stan computes with concrete values.  When we run a Stan
program, it produces a sequence of simulated values for each random
variable in the program.  We index sequences of values with
parenthesized superscripts.  For example, we might write $M$ simulated
values of the random variable $Y$ as $y^{(0)}, \ldots, y^{(M-1)}$.

### Random seeds

In our sampling code, we fixed a random seed, `1234`, which determines
the sequence of random numbers generated In practical sampling code,
this means its value is dependent on the state of the underlying
random number generator.  This mirrors the way random variables are
defined in measure theory as functions from sample spaces to values.  



## Expectation, variance, and standard deviation

The *expectation* of a random variable is defined as its average
value. For example, if $Y$ is a random variable, we write its
expectation as $\mathbb{E}[y]$.  We can calculate approximate
expectation values by averaging samples.  If $y^{(0)}, \ldots,
y^{(M-1)}$ is a sequence of $M$ simulated values for $Y$, then the *expectation* of 
$$
\mathbb{E}[Y] \approx \frac{1}{M} \sum_{m < M} y^{(m)}.
$$


### Variance and standard deviation

The *variance* of a random variable is defined as its average squared
difference from its expected value.  This is also an expectation.
$$
\textrm{var}[Y] = \mathbb{E}[(Y - \mathbb{E}[Y])^2] \approx \frac{1}{M} \sum_{m < M} (y^{(m)} - \mathbb{E}[Y])^2.
$$
The units of variance are the units of $Y$ squared, which makes it inconvenient for informal reasoning.

The *standard deviation* of a random variable is defined to be the
square root of its variance.
$$
\textrm{sd}[Y] = \sqrt{\textrm{var}[Y]}.
$$


### Quantiles

If $\alpha \in [0, 1]$, we say that the $\alpha$-*quantile* of a the
random variable $Y$ is the value $y$ such that the probability that
$Y$ is less than or equal to $y$ is $\alpha$. That is, $y$ is the
$\alpha$-quantile of $Y$ if $\textrm{Pr}[Y < y] = \alpha$. We can
estimate the $\alpha$-quantile through sampling as
$$
\textrm{quantile}(Y, \alpha)
\approx y^{(\lfloor \alpha \cdot M \rfloor)}.
$$

