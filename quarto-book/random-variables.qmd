# Random Variables

Stan is a probabilistic programming language in the sense that its
variables can be random as well as deterministic.  Each time Stan is
run, these random variables can take on different values.

## Simulating a fair coin flip

We will start with the canonical "Hello, world!" example for
statistics, a coin flip.

### Stan program for a coin flip

Here's the Stan program.  When we list Stan
programs, we title them with the file name in which they can be found
in the source distribution.

##### `flip.stan`
```stan
generated quantities {
  int<lower=0, upper=1> y = bernoulli_rng(0.5);
}
```

This trivial Stan program assigns the variable `y` randomly, with a
50% chance of a 1 value and 50% chance of a 0 value.  The program
begins with a *block declaration*.  The *generated quantities block*
here is used to generate random quantities based on the state of other
program variables.  The variable `y` is declared to be an integer with
the type `int` and constrained to fall between 0 and 1 (inclusive)
with `lower=0` and `upper=1`.  The variable's value is the result of
executing the function `bernoulli_rng` applied to the argument
expression `0.5`.  The value of `bernoulli_rng` is random, returning 1
with the specified probability of 0.5 (a 50% chance) and returning 0
with probability 0.5.


### Executing Stan through Python

We use Python to run the code and show the result of the simulated
coin flip.  First, we are going to import cmdstanpy and then turn off
its intrinsic logger to avoid cluttering our output with feedback messages intended for the console.


```{python}
#| output: false
from cmdstanpy import CmdStanModel

import logging
cmdstanpy_logger = logging.getLogger("cmdstanpy")
cmdstanpy_logger.disabled = True
```

Next, we compile the Stan model from its source `flip.stan` using the
constructor for the class `CmdStanModel`.

```{python}
model = CmdStanModel(stan_file = "flip.stan")
```

Next, we use the `sample()` method on the model class to sample values
from the program.

```{python}
fit = model.sample(seed=1234,
                   show_progress=False,
                   show_console = False)
```

The `seed` argument determines a random seed.  If we rerun a Stan
program with the same random seed, we get the same sequence of
pseudo-random simulation values.


### Extracting fitted results

After fitting a model, we can extract the draws for `y` from the `fit`
object.

```{python}
df = fit.draws_pd()
y = df['y']
```

For example, here's the result of the first draw.

```{python}
print("first flip = {0:.0f}".format(y[0]))
```

Continuing past the first simulation, here are the first dozen
simulated values.


```{python}
for i in range(12):
    print("y({0:d}) = {1:.0f}".format(i, y[i]), end=", ")
    if i == 5: print("")
print("...")
```

We see that some of the simulated values are 1 and some are 0.  These
do not correspond to flipping a dozen different coins or the same coin
a dozen times.  Instead, the simulated values represent different
possible values for a single flip of a single coin.


## Simulating a fair die

The same way we could generate two variables uniformly, we can
define a random variable whose value falls between 1 and 6 randomly.  Here is the Stan code.

`die.stan`
```stan
generated quantities {
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
}
```

Here, we have a lower bound of 1 and upper bound of 6 (inclusive), and
we use a categorical random number generator that assigns a
probability of `1 / 6` to each outcome (we write the numbers using
decimal places to avoid reduction to integer arithmetic, which
rounds toward zero).

```{python}
#| code-fold: true
model_die = CmdStanModel(stan_file = "die.stan")
fit_die = model_die.sample(seed=1234, show_progress=False, show_console=False)
z = fit_die.draws_pd()["z"]
for i in range(10):
  print("z({0:d}) = {1:.0f}".format(i, z[i]), end=", ")
  if i == 4: print("")
print("...")
```


## Functions on random variables

If $X$ is a random variable and $Y$ is a random variable, then so is
$Z = X + Y$.  We can simulate values for $Z$ by simulating a value for
$X$ and simulating a value for $Y$ and then adding them.  For
instance, if we want the result of rolling two dice, we can do that
with the following Stan program.

`dice.stan`
```stan
generated quantities {
  int<lower=1, upper=6> x = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=1, upper=6> y = categorical_rng(rep_vector(1.0 / 6.0, 6));
  int<lower=2, upper=12> z = x + y;
}
```

The Stan program defines two variables, `x` and `y`, modeling each as
a fair die, and defines the variable `z` to be their sum.  Here are 20
simulations of `z`.

```{python}
#| code-fold: true
model_dice = CmdStanModel(stan_file = "dice.stan")
fit_dice = model_dice.sample(seed=1234, show_progress=False, show_console=False)
z = fit_dice.draws_pd()["z"]
for i in range(20):
  print("z({0:2d}) = {1:2.0f}".format(i, z[i]), end=", ")
  if (i + 1) % 5 == 0: print("")
print("...")
```

While every outcome of throwing a single fair die is equally likely,
this is no longer true when throwing two dice and taking their sum.
As you can see from the example, intermediate outcomes like 7 and 8 occur
more frequently than extreme values like 11 and 12.


## Event probabilities

A general condition on a random variable is called an *event*.
Suppose $Z$ is a random variable representing the outcome of a fair
roll of two six-sided dice.  Events include examples such as $Z$ being
equal to 7, or being less than or equal to 5, or being odd.
Informally, an event $A$ is just a subset of possible outcomes for a
random variable.^[Defining the collection of events more formally
requires the notion of a $\sigma$-algebra from measure theory, which
is beyond the scope of this book.]  For example, the conditions on
random variables we considered above correspond to events $A$,
$B$, and $C$, where follows. 

* $Z = 7$:  $\quad A = \{ 7 \}$,
* $Z \leq 5 \ $: $\quad B = \{ 2, 3, 4, 5 \}$, and 
* $Z \textrm{ odd}$: $\quad C = \{ 3, 5, 7, 9, 11 \}$.

If $E$ is an event, we will write $\textrm{Pr}[E]$ for the
*probability* of that $E$ occurs.^[We use the higher-order function
notation of square brackets because events are sets of outcomes.]

In the case of dice, probabilities are calculated as the sum of the
probabilities of the individidual outcomes.  For example,
$$
\textrm{Pr}[Z \leq 5]
= \textrm{Pr}[Z = 2] + \textrm{Pr}[Z = 3] + \textrm{Pr}[Z = 4] + \textrm{Pr}[Z = 5].
$$
Probabilities are scaled so that they so that they fall between 0 and
1 inclusive, i.e., $\textrm{Pr}[A] \in [0, 1]$.

## Estimating probabilities with simulation

Although we could calculate the probabilities of each outcome of
tossing two fair dice analytically, we first tackle the problem
through simulation.  With simulation, all we need to do is run
multiple simulations and look at simulated frequencies.

We begin with draws $z^{(1)}$, \ldots, $z^{(M)}$ of $Z$ corresponding
to a fair roll of two six-sided dice. To estimate the probabilty of an
outcome, for example $\textrm{Pr}[Z = 7]$, we can use the proportion
of draws in which $z^{(m)} = 7$.  More formally, we will write this as
$$
\textrm{Pr}[Z = 7] = \frac{1}{M} \sum_{m=1}^M \textrm{I}(z^{(m)} = 7).
$$

We can calculate these estimates for all values of $Z$ in a short loop.

```{python}
import numpy as np
M = len(z)
counts = np.zeros(11)
for m in range(M):
  counts[int(z[m]) - 2] += 1
pr_z_eq = counts / M
```

Printing the results, we see that not every outcome is equally probable.

```{python}
#| code-fold: true
for n in range(len(pr_z_eq)):
  print("Pr[Z = {0:2d}] = {1:5.3f}".format(n + 2, pr_z_eq[n]), end="   ")
  if ((n + 1) % 3 == 0): print("")
```

A more idiomatic approach is to use a `Counter` followed by a list
comprehension to carry out the arithmetic.

```{python}
#| code-fold: true
from collections import Counter
counts = Counter(z)
M = len(z)
pr_z_eq = [counts[n] / M for n in range(2, 13)]
```

In @fig-two-d-six We can plot our output using a bar chart, where the heights of the
bars are proportional to the probability of the outcome listed on the
axis.  

```{python}
#| label: fig-two-d-six
#| fig-cap: "Probabilities of outcomes for rolling two six-sided dice"
#| code-fold: true
import numpy as np
import pandas as pd
import plotnine as pn
probs = []
for n in range(2, 13):
  probs.append(counts[n] / len(z))

roll_probs = { "roll": range(2, 13),
               "estimated probability": probs }

df = pd.DataFrame(roll_probs)

plot = pn.ggplot(df, pn.aes(x = 'roll', y = 'estimated probability')) \
  + pn.geom_bar(stat="identity") \
  + pn.scale_x_continuous(breaks=range(2, 13))
plot.draw();  # semicolon prevents unwanted text echo
```

We write "estimated probability" on the vertical axis of this plot to
emphasize that we are using computational methods to approximate the
true value rather than analytically computing it with higher
precision.

### Calculating dice probabilities analytically

In the case of two dice, it's straightforward to calculate the
probability of each outcome analytically.  Because we have two
six-sided dice, there are 36 possible combinations $(d_1, d_2)$, where
$d_1$ is the result of the first die and $d_2$ the second. Each
outcome $(d_1, d_2)$ is equally probable, so the total probability of
an outcome is the number of ways it can be produced divided by the
total number of outcomes.  Table @tbl-2d6 shows the probabilities for
results 2--7.

| result | combinations  | probability |
|:-:|:-:|:-:|
| 2 | (1, 1) | 1/36 |
| 3 | (1, 2) &nbsp;  (2, 1) | 2/36 |
| 4 | (1, 3) &nbsp;  (2, 2) &nbsp;  (3, 1) | 3/36 |
| 5 | (1, 4) &nbsp;  (2, 3) &nbsp;  (3, 2) &nbsp;  (4, 1) | 4/36 |
| 6 | (1, 5) &nbsp;  (2, 4) &nbsp;  (3, 3) &nbsp;  (4, 2) &nbsp;  (5 1) | 5/36 |
| 7 | (1, 6) &nbsp;  (2, 5) &nbsp;  (3, 4) &nbsp;  (4, 3) &nbsp;  (5, 2) &nbsp;  (6, 1) | 6/36 |
: Combinations of two six-sided dice leading to result (their sum), and the associated probability. Results greater than 7 are symmetric, with 8 matching 6, 9 matching 5, $\ldots$, and 12 matching 2. {#tbl-2d6}

These results are very close to our estimated results, though it is
evident the simulation provides only about two significant digits of
accuracy compared to the analytical result.




## Simulating stick breaking

Imagine we have a stick that's twenty units long and we break it in a
random place.  That corresponds to generating a continuous random
number in the range $[0, 20]$.  We can simulate this random process with Stan as follows.

`stick.stan`
```stan
generated quantities {
  real<lower=0, upper=20> alpha = uniform_rng(0, 20);
}
```

In contrast to our previosu example, we declare our variable to be real-valued with the keyword `real`.  We also use a uniform random number generator, which generates a real number in the specified range at random.

We will compile the model and sample in a single code block this time.

```{python}
model_stick = CmdStanModel(stan_file = "stick.stan")
fit_stick = model_stick.sample(seed=1234, show_progress=False, show_console=False)
alpha = fit_stick.draws_pd()["alpha"]
for i in range(5):
  print("alpha({0:d}) = {1:5.2f}".format(i, alpha[i]))
```

We have only printed the first two decimal places of each number, but we can see that we are generating real values between 0 and 20.




## Random variables

In probability theory notation, random variables are conventionally
written as capital letters.  For example, we might let $Y$ be the
random variable corresponding to the result of a specific fair coin
flip.  Values for random variables are conventionally written using
lower case.  For example, $y = 1$ is a particular value of the
random variable $Y$.

The variable `y` in our sample program is written in lower case
because Stan computes with concrete values.  When we run a Stan
program, it produces a sequence of simulated values for each random
variable in the program.  We index sequences of values with
parenthesized superscripts.  For example, we might write $M$ simulated
values of the random variable $Y$ as $y^{(0)}, \ldots, y^{(M-1)}$.

### Random seeds

In our sampling code, we fixed a random seed, `1234`, which determines
the sequence of random numbers generated In practical sampling code,
this means its value is dependent on the state of the underlying
random number generator.  This mirrors the way random variables are
defined in measure theory as functions from sample spaces to values.  



## Expectation, variance, and standard deviation

The *expectation* of a random variable is defined as its average
value. For example, if $Y$ is a random variable, we write its
expectation as $\mathbb{E}[y]$.  We can calculate approximate
expectation values by averaging samples.  If $y^{(0)}, \ldots,
y^{(M-1)}$ is a sequence of $M$ simulated values for $Y$, then the *expectation* of 
$$
\mathbb{E}[Y] \approx \frac{1}{M} \sum_{m < M} y^{(m)}.
$$


### Variance and standard deviation

The *variance* of a random variable is defined as its average squared
difference from its expected value.  This is also an expectation.
$$
\textrm{var}[Y] = \mathbb{E}[(Y - \mathbb{E}[Y])^2] \approx \frac{1}{M} \sum_{m < M} (y^{(m)} - \mathbb{E}[Y])^2.
$$
The units of variance are the units of $Y$ squared, which makes it inconvenient for informal reasoning.

The *standard deviation* of a random variable is defined to be the
square root of its variance.
$$
\textrm{sd}[Y] = \sqrt{\textrm{var}[Y]}.
$$


### Quantiles


If $\alpha \in [0, 1]$, we say that the $\alpha$-*quantile* of a the
random variable $Y$ is the value $y$ such that the probability that
$Y$ is less than or equal to $y$ is $\alpha$.  That is, $y$ is the
$\alpha$-quantile of $Y$ if $\textrm{Pr}[Y < y] = \alpha$.  We can estimate the $\alpha$-quantile through sampling as
$$
\textrm{quantile}(Y, \alpha) \approx y^{(\lfloor \alpha \cdot M \rfloor)}.
$$

